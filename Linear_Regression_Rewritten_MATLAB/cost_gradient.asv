function [J, theta, all_J] = cost_gradient(theta, X, y, m, alpha, iteration)
% Formula for the hypothesis
h = X * theta;
% Formula for the Cost Function
J = (1 / (2 .* m)) * sum((h - y).^2);

all_J = zeros(num_iters, 1)

%Formula for gradient
for iter = 1 : iteration
    temp1 = theta(1) - (alpha * (1 / m) * sum(((X * theta) - y) .* X(:, 1)));
    temp2 = theta(2) - (alpha * (1 / m) * sum(((X * theta) - y) .* X(:, 2)));

    theta(1) = temp1;
    theta(2) = temp2;
    
    %Find J for each iteration
    h = X * theta;
    % Formula for the Cost Function
    all_J(iter) = (1 / (2 .* m)) * sum((h - y).^2);

end

end



